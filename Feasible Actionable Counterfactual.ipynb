{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import lime\n",
    "import re\n",
    "import shap\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from lime.lime_tabular import LimeTabularExplainer\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "cc_data = pd.read_excel('default of credit card clients.xls')\n",
    "\n",
    "# missing values\n",
    "cc_data.dropna(inplace=True)\n",
    "print(cc_data.shape)\n",
    "cc_data.head(3)\n"
   ],
   "id": "edf63a46cce1bf63"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# dataset preprocessing\n",
    "\n",
    "# dataset and target variable\n",
    "dc_data = cc_data.drop(columns=['default payment next month'], axis=1)\n",
    "# 1: yes; 0: no\n",
    "m_payment = cc_data['default payment next month']\n",
    "\n",
    "# scale the dataset\n",
    "data_columns = dc_data.columns\n",
    "scaler =MinMaxScaler()\n",
    "dc_data_scaled = scaler.fit_transform(dc_data)\n",
    "dc_data = pd.DataFrame(dc_data_scaled, columns= data_columns)\n",
    "\n",
    "# split dataset 70:30\n",
    "x_train, x_test, y_train, y_test = train_test_split(dc_data, m_payment, test_size=0.3, random_state=45)"
   ],
   "id": "2af61eb14441979d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Using SMOTE to balance the dataset because this dataset is imbalanced, making it biased to the majority class.\n",
    "# WHY? Improves model's fairness, boosts performance and automates variable selection while preserving the structure of the dataset.\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "CANDIDATE_VARS = [\n",
    "    (\"x_train\", \"y_train\"),\n",
    "    (\"x_tr\", \"y_tr\"),\n",
    "    (\"x_train_enc\", \"y_train_enc\"),\n",
    "    (\"x_train_scaled\", \"y_train\"),\n",
    "    (\"x_train_final\", \"y_train_final\"),\n",
    "]\n",
    "\n",
    "# locate the valid training variables\n",
    "x_train_var, y_train_var = None, None\n",
    "for xv, yv in CANDIDATE_VARS:\n",
    "    if xv in globals() and yv in globals():\n",
    "        x_train_var, y_train_var = xv, yv\n",
    "        break\n",
    "\n",
    "if x_train_var is None:\n",
    "    raise NameError(\"Could not find X_train / y_train. Edit [MOD A] to set your variable names.\")\n",
    "\n",
    "print(f\"[MOD A] Using training vars: {x_train_var}, {y_train_var}\")\n",
    "print(\"[MOD A] Class distribution before:\", Counter(globals()[y_train_var]))\n",
    "\n",
    "\n",
    "# apply the SMOTE technique\n",
    "try:\n",
    "    from imblearn.over_sampling import SMOTE\n",
    "except Exception as e:\n",
    "    raise ImportError(\"imblearn (imbalanced-learn) is required for SMOTE.\") from e\n",
    "\n",
    "sm = SMOTE(random_state=42, k_neighbors=5)\n",
    "X_train_sm, y_train_sm = sm.fit_resample(globals()[x_train_var], globals()[y_train_var])\n",
    "\n",
    "print(\"[MOD A] Class distribution after:\", Counter(y_train_sm))\n",
    "\n",
    "# dataset column preservation\n",
    "import pandas as pd\n",
    "if hasattr(globals()[x_train_var], \"columns\"):\n",
    "    X_train_sm = pd.DataFrame(X_train_sm, columns=globals()[x_train_var].columns)"
   ],
   "id": "78179c6b70aea56"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Applying class weight to balance the dataset, as it tells the model to pay attention to the minority class during training.\n",
    "# classification algorithms to train the model for the predicted outcome - SVM, RF, Logistic Regression\n",
    "\n",
    "class_weights_strategy = \"balanced\"   # or a dict like {0: 1.0, 1: 2.5}\n",
    "\n",
    "def make_model(model_type=\"logreg\", **kwargs):\n",
    "    if model_type == \"logreg\":\n",
    "        from sklearn.linear_model import LogisticRegression\n",
    "        return LogisticRegression(class_weight=class_weights_strategy, max_iter=1000, **kwargs)\n",
    "    elif model_type == \"svm\":\n",
    "        from sklearn.svm import SVC\n",
    "        return SVC(class_weight=class_weights_strategy, probability=True, **kwargs)\n",
    "    elif model_type == \"rf\":\n",
    "        from sklearn.ensemble import RandomForestClassifier\n",
    "        return RandomForestClassifier(class_weight=class_weights_strategy, n_estimators=300, random_state=42, **kwargs)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported model_type. Extend make_model().\")"
   ],
   "id": "c44cfcde9e99dbb9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#  Tuning the model through calibration and threshold tuning,\n",
    "# Calibration ensures the predicted probabilities to reflect the true likelihoods; threshold tuning adjusts the cutoff for converting probabilities into class labe;s\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "# using the SMOTE and class weights function\n",
    "if 'base_model' not in globals():\n",
    "    base_model = make_model(\"svm\", kernel=\"rbf\", C=1.0, gamma=\"scale\")\n",
    "\n",
    "X_for_fit = X_train_sm if 'X_train_sm' in globals() else globals()[x_train_var]\n",
    "y_for_fit = y_train_sm if 'y_train_sm' in globals() else globals()[y_train_var]\n",
    "\n",
    "X_cal_train, X_cal_valid, y_cal_train, y_cal_valid = train_test_split(X_for_fit, y_for_fit, test_size=0.2, random_state=42, stratify=y_for_fit)\n",
    "\n",
    "base_model.fit(X_cal_train, y_cal_train)\n",
    "\n",
    "\n",
    "# wrapping the base model in the calibration technique\n",
    "# isotonic: more flexible but requires more data; OR sigmoid: simpler, works well with less data\n",
    "# uses cross-validation to carry out calibration on the validation set and avoid overfitting on the dataset.\n",
    "\n",
    "calibration_method = \"isotonic\"  # or \"sigmoid\"\n",
    "calibrated_model = CalibratedClassifierCV(estimator=base_model, method=calibration_method, cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42))\n",
    "calibrated_model.fit(X_cal_valid, y_cal_valid)\n",
    "\n",
    "# threshold tuning\n",
    "def tune_threshold(model, X, y):\n",
    "    proba = model.predict_proba(X)[:, 1]\n",
    "    thresholds = np.linspace(0.1, 0.9, 81)\n",
    "\n",
    "    from sklearn.metrics import f1_score\n",
    "    best_t, best_f1 = 0.5, -1.0\n",
    "\n",
    "    for t in thresholds:\n",
    "        yhat = (proba >= t).astype(int)\n",
    "        f1 = f1_score(y, yhat, zero_division=0)\n",
    "        if f1 > best_f1:\n",
    "            best_t, best_f1 = t, f1\n",
    "    return best_t\n",
    "\n",
    "THRESHOLD = tune_threshold(calibrated_model, X_cal_valid, y_cal_valid)\n",
    "print(f\"[MOD C] Using decision threshold: {THRESHOLD:.3f}\")"
   ],
   "id": "bef68455ec0569f8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Classification algorithms to train the model for the predicted outcome\n",
    "# Run to get the accuracy, recall and F1 score - IF NEEDED\n",
    "\n",
    "# ML model A\n",
    "svm_model = make_model(\"svm\", kernel=\"rbf\", C=1.0, gamma=\"scale\")\n",
    "svm_model.fit(X_train_sm, y_train_sm)\n",
    "\n",
    "# ML model B\n",
    "rf_model = make_model(\"rf\")\n",
    "rf_model.fit(X_train_sm, y_train_sm)\n",
    "\n",
    "# ML model C\n",
    "logr_model = make_model(\"logreg\")\n",
    "logr_model.fit(X_train_sm, y_train_sm)\n",
    "#\n",
    "#\n",
    "# predict\n",
    "rf_pred = rf_model.predict(x_test)\n",
    "svm_pred = svm_model.predict(x_test)\n",
    "logr_pred = logr_model.predict(x_test)\n",
    "\n",
    "\n",
    "# RF\n",
    "accuracy = accuracy_score(y_test, rf_pred)\n",
    "print(\"Random Forest Classifier\")\n",
    "print(classification_report(y_test, rf_pred))\n",
    "print('\\n')\n",
    "\n",
    "# SVM\n",
    "accuracy = accuracy_score(y_test,svm_pred)\n",
    "print(\"SVM Classifier\")\n",
    "print(classification_report(y_test,svm_pred))\n",
    "print('\\n')\n",
    "\n",
    "# LogR\n",
    "accuracy = accuracy_score(y_test,logr_pred)\n",
    "print(\"Log R Classifier\")\n",
    "print(classification_report(y_test, logr_pred))\n",
    "print('\\n')\n"
   ],
   "id": "a98c5a10793facbc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Feature importance selection\n",
    "# SHAP: highlight the global importance\n",
    "\n",
    "# performance boost\n",
    "background_data = X_train_sm.sample(n=100, random_state=42)\n",
    "masker = shap.maskers.Independent(background_data)\n",
    "\n",
    "# # svm model (output is the same when ran for the logistic regression model)\n",
    "svm_shap_explainer = shap.Explainer(svm_model.predict_proba, masker)\n",
    "svm_s_values = svm_shap_explainer.shap_values(x_test[:100])"
   ],
   "id": "1bc7b03ea1852985"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# # global feature importance -\n",
    "# SHAP\n",
    "# Mean absolute SHAP value per feature\n",
    "\n",
    "# svm model\n",
    "svm_s_values_total = svm_s_values[:, :, 1]\n",
    "svm_feature_importance = pd.DataFrame({\n",
    "        'feature': x_test.columns,\n",
    "        'importance': np.abs(svm_s_values_total).mean(axis=0)\n",
    "        }).sort_values(by='importance', ascending=False)    # the higher the number, the more influential it is\n",
    "\n",
    "print('SVM: SHAP\\n', svm_feature_importance)\n",
    "\n",
    "svm_shap_values = svm_feature_importance"
   ],
   "id": "4f3be0af36733540"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# causal graph from selected features\n",
    "# uses the FCI and DirectLINGAM method\n",
    "# FCI: constraint-based method that infers causal skeletons from conditional independence tests\n",
    "# DirectLINGAM: functional method that assumes linear non-guassian relationships to infer causal directions\n",
    "# both: FCI handles latent cofounders, and LiNGAM gives directionality; improves reliability of causal edges\n",
    "# output: A directed causal graph that is grounded in both statistical independence and functional modeling, that provides causal validity\n",
    "\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from causallearn.search.ConstraintBased.FCI import fci\n",
    "from causallearn.utils.cit import fisherz\n",
    "from lingam import DirectLiNGAM\n",
    "\n",
    "# important features\n",
    "important_features = svm_shap_values.iloc[:, 0].astype(str).tolist()\n",
    "# identified features from the dataset\n",
    "f_data = cc_data[important_features]\n",
    "\n",
    "print(\"Selected columns:\", important_features)\n",
    "print(\"f_data shape:\", f_data.shape)\n",
    "\n",
    "# Remove rows with missing values (FCI and Lingam don't handle NaNs well)\n",
    "f_data = f_data.dropna().reset_index(drop=True)\n",
    "\n",
    "# convert to numeric datatype\n",
    "f_data = f_data.apply(lambda col:pd.factorize(col)[0] if col.dtypes == 'object' else col)\n",
    "\n",
    "# Normalize (required for Lingam to perform well)\n",
    "norm_f_data = ((f_data - f_data.mean()) / f_data.std())\n",
    "\n",
    "\n",
    "########################################\n",
    "# Step 1: Constraint-Based Search (FCI)\n",
    "########################################\n",
    "data_d = norm_f_data.to_numpy()\n",
    "fci_result = fci(data_d, fisherz, alpha=0.01, verbose=False)\n",
    "fci_graph = fci_result[0]\n",
    "\n",
    "print(\"\\nFCI Skeleton Edges:\")\n",
    "for edge in fci_graph.get_graph_edges():\n",
    "    i = edge.node1\n",
    "    j = edge.node2\n",
    "    print(f\"{i} <-> {j}\")\n",
    "    # print(f\"{f_data.columns[i]} <-> {f_data.columns[j]}\")\n",
    "\n",
    "########################################\n",
    "# Step 2: Functional Causal Discovery (DirectLiNGAM)\n",
    "########################################\n",
    "model = DirectLiNGAM()\n",
    "model.fit(data_d)\n",
    "lingam_adj = model.adjacency_matrix_\n",
    "\n",
    "print(\"\\nLiNGAM Causal Edges:\")\n",
    "for i in range(len(lingam_adj)):\n",
    "    for j in range(len(lingam_adj)):\n",
    "        if lingam_adj[i, j] != 0:\n",
    "            print(f\"{f_data.columns[i]} --> {f_data.columns[j]}\")\n",
    "\n",
    "########################################\n",
    "# Step 3: Merge FCI Skeleton with LiNGAM directions\n",
    "########################################\n",
    "combined_graph = nx.DiGraph()\n",
    "\n",
    "# Add nodes\n",
    "combined_graph.add_nodes_from(f_data.columns)\n",
    "# map edge_nodes to a feature name\n",
    "column_map = {f\"X{i}\": col for i, col in enumerate (f_data.columns)}\n",
    "# Add FCI undirected edges first\n",
    "columns = list(f_data.columns)\n",
    "\n",
    "for edge in fci_graph.get_graph_edges():\n",
    "    i_name = edge.node1.get_name()\n",
    "    j_name = edge.node2.get_name()\n",
    "    if i_name in column_map and j_name in column_map:\n",
    "        i = column_map[i_name]\n",
    "        j = column_map[j_name]\n",
    "        combined_graph.add_edge(i, j, causal_source='fci')\n",
    "\n",
    "# Add directions from LiNGAM if they match FCI edges\n",
    "for i in range(len(lingam_adj)):\n",
    "    for j in range(len(lingam_adj)):\n",
    "        if lingam_adj[i, j] != 0:\n",
    "            src, tgt = f_data.columns[i], f_data.columns[j]\n",
    "            if combined_graph.has_edge(src, tgt):\n",
    "                # Already exists: keep as directed\n",
    "                combined_graph[src][tgt]['weight'] = 'lingam'\n",
    "            elif combined_graph.has_edge(tgt, src):\n",
    "                # Reverse direction if undirected in FCI\n",
    "                combined_graph.remove_edge(tgt, src)\n",
    "                combined_graph.add_edge(src, tgt, causal_source='lingam')\n",
    "            else:\n",
    "                combined_graph.add_edge(src, tgt, causal_source='lingam')\n",
    "\n",
    "# ########################################\n",
    "# # Step 4: Visualize the Combined Graph\n",
    "# ########################################\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# pos = nx.spring_layout(combined_graph, seed=42, weight = None)\n",
    "# edge_labels = nx.get_edge_attributes(combined_graph, 'causal_source')\n",
    "#\n",
    "# nx.draw(combined_graph, pos, with_labels=True, node_size=2000, node_color='lightblue', font_size=10, arrowsize=20)\n",
    "# nx.draw_networkx_edge_labels(combined_graph, pos, edge_labels=edge_labels)\n",
    "# plt.title(\"Combined Causal Graph (FCI + LiNGAM)\")\n",
    "# plt.show()\n"
   ],
   "id": "27d92790d1ae0899"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# feature extraction - immutable and actionable features\n",
    "\n",
    "def detect_actionable_immutable(df):\n",
    "    immutable_f = {col for col in df.columns if any(word in col.lower() for word in [\"sex\", \"age\",  \"ID\", \"gender\", \"race\", \"marital-status\", 'ethnicity', 'birth', 'native-country', 'education'])}\n",
    "    actionable_f = set(df.columns) - immutable_f - {\"class\"}\n",
    "    return sorted(actionable_f), sorted(immutable_f)\n",
    "\n",
    "actionable_f, immutable_f = detect_actionable_immutable(cc_data)\n",
    "\n",
    "# # Reserve 1–2 intermediate features for plausibility\n",
    "# These must have parents in the graph\n",
    "plausibility_nodes = ['PAY_AMT1', 'BILL_AMT4']  # <- adjust based on your graph\n",
    "\n",
    "# Remove from actionable/immutable\n",
    "actionable_f = [f for f in actionable_f if f not in plausibility_nodes]\n",
    "immutable_f = [f for f in immutable_f if f not in plausibility_nodes]\n"
   ],
   "id": "aa554a614e05154b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# SEM to enforce causal validity for the counterfactual generation (validate causal assumption)\n",
    "# DAG: a graph with directed edges and no cycles, represents causal flow\n",
    "# SEM: a set of equations that models each variable as a function of its causal parents\n",
    "\n",
    "# deterministic\n",
    "def estimate_sems(df, graph):\n",
    "    sem = {}\n",
    "    for node in nx.topological_sort(graph):\n",
    "        parents = list(graph.predecessors(node))\n",
    "        if not parents:\n",
    "            sem[node] = ([], 0)\n",
    "        else:\n",
    "            X = df[parents]\n",
    "            y = df[node]\n",
    "            beta = np.linalg.pinv(X.values) @ y.values\n",
    "            sem[node] = (parents, beta)\n",
    "    return sem\n",
    "\n",
    "# probabilistic\n",
    "# captures noise and variability, required to calculate the plausibility of the generated counterfactual\n",
    "def estimate_p_sems(df, graph):\n",
    "    sem = {}\n",
    "    for node in nx.topological_sort(graph):\n",
    "        parents = list(graph.predecessors(node))\n",
    "\n",
    "        if not parents:\n",
    "            mu = df[node].mean()\n",
    "            sigma = df[node].std()\n",
    "            sem[node] = ([], np.array([mu]), sigma)  # store mean as beta-like vector\n",
    "        else:\n",
    "            X = df[parents]\n",
    "            y = df[node]\n",
    "            beta = np.linalg.pinv(X.values) @ y.values\n",
    "            residuals = y - X @ beta\n",
    "            sigma = np.std(residuals)\n",
    "            sem[node] = (parents, beta, sigma)\n",
    "    return sem\n",
    "\n",
    "\n",
    "# extract a DAG with only LiNGAM-directed edges (to ensure acyclicity)\n",
    "DAG_graph = nx.DiGraph(\n",
    "    (u, v, d) for u, v, d in combined_graph.edges(data=True)\n",
    "    if d.get(\"causal_source\") == \"lingam\")\n",
    "\n",
    " #Optional: Check if DAG is valid\n",
    "assert nx.is_directed_acyclic_graph(DAG_graph), \"Graph must be a DAG for SEM estimation\"\n",
    "\n",
    "# calling the SEM - follows linear_regression\n",
    "sem_equations = estimate_p_sems(norm_f_data, DAG_graph)"
   ],
   "id": "65d5e86a812b5013"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# SEM - conversion to callable functions, so it can be used for the counterfactual generation\n",
    "# Enables modular, reusable causal modeling to ensure causal dependencies are respected (causal plausibility enforcement)\n",
    "\n",
    "# a deterministic SEM for the causal plausibility - returns a single value\n",
    "def make_linear_sem_function(parents, beta):\n",
    "    def sem_func(X_parent_values):\n",
    "        return X_parent_values @ beta\n",
    "    return sem_func\n",
    "\n",
    "# a probabilistic SEM for the causal plausibility - returns a distribution\n",
    "def make_probabilistic_sem_function(parents, beta, sigma):\n",
    "    def sem_func(X_parent_values):\n",
    "        if not parents:\n",
    "            mu = beta[0] if isinstance(beta, np.ndarray) else beta\n",
    "            return np.full((X_parent_values.shape[0], 1), mu), sigma\n",
    "        else:\n",
    "            mu = X_parent_values @ beta\n",
    "            return mu, sigma\n",
    "    return sem_func\n",
    "\n",
    "default_sigma = 0.5\n",
    "sem_functions = {}\n",
    "\n",
    "for node, values in sem_equations.items():\n",
    "    if len(values) == 3:\n",
    "        # Probabilistic SEM with parents\n",
    "        parents, beta, sigma = values\n",
    "        sem_func = make_probabilistic_sem_function(parents, beta, sigma)\n",
    "        sem_functions[node] = (sem_func, parents)\n",
    "    elif len(values) == 2:\n",
    "        # Root node: values = ([], mu)\n",
    "        _, beta = values\n",
    "        sigma = default_sigma  # assign a default sigma to root node\n",
    "        sem_func = make_probabilistic_sem_function([], beta, sigma)\n",
    "        sem_functions[node] = (sem_func, [])\n",
    "    else:\n",
    "        raise ValueError(f\"Unexpected SEM format for node {node}: {values}\")\n",
    "\n",
    "for node, val in sem_functions.items():\n",
    "    print(node, type(val), len(val))\n"
   ],
   "id": "1c8756f0fa0c0789"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ----------------------------\n",
    "# Helper functions for mutation\n",
    "# ----------------------------\n",
    "# generates candidates for mutation\n",
    "# maintains structural validity, avoids premature convergence, prevents invalid mutations, and provides efficient mutation of multiple candidates\n",
    "# encourages exploration and robustness\n",
    "\n",
    "# perturbs actionable features of a single data point and propagate changes using SEM to maintain causal consistency\n",
    "# ensures mutation are realistic and causally plausible and used for generating diverse counterfactuals\n",
    "\n",
    "# balances exploration (diversity) and causal realism\n",
    "def batch_mutate_diverse(X, actionable_idx, sem_functions, step=0.05,\n",
    "                         rng=None, mutation_rate=0.1,\n",
    "                         categorical_idx=None):\n",
    "    \"\"\"\n",
    "    Mutates a batch of solutions X according to SEM functions,\n",
    "    ensuring diversity in each generation.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : np.ndarray\n",
    "        Candidate solutions (n_solutions, n_features)\n",
    "    actionable_idx : list[int]\n",
    "        Indices of actionable features.\n",
    "    sem_functions : dict\n",
    "        Mapping feature index -> (sem_func, parents).\n",
    "    step : float\n",
    "        Magnitude of perturbation for continuous root nodes.\n",
    "    rng : np.random.Generator\n",
    "        Random number generator for reproducibility.\n",
    "    mutation_rate : float\n",
    "        Probability of mutating each actionable feature.\n",
    "    categorical_idx : set[int] or None\n",
    "        Indices of categorical actionable features (optional).\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng() if rng is None else rng\n",
    "    categorical_idx = set() if categorical_idx is None else set(categorical_idx)\n",
    "\n",
    "    X_new = X.copy()\n",
    "\n",
    "    for i in range(X.shape[0]):\n",
    "        updated = set()\n",
    "\n",
    "        # ✅ Guarantee at least one actionable feature will mutate\n",
    "        forced_mutation = rng.choice(actionable_idx)\n",
    "\n",
    "        for j, (sem_func, parents) in sem_functions.items():\n",
    "            if j not in actionable_idx:\n",
    "                continue  # skip immutable features\n",
    "\n",
    "            do_mutate = (rng.random() < mutation_rate) or (j == forced_mutation)\n",
    "\n",
    "            if do_mutate:\n",
    "                if parents:\n",
    "                    # Dependent node: recompute from parents + small noise for diversity\n",
    "                    parent_vals = X_new[i, parents]\n",
    "                    val = sem_func(parent_vals)\n",
    "                    if j not in categorical_idx:\n",
    "                        val += rng.normal(0, step)  # add diversity noise\n",
    "                    X_new[i, j] = val\n",
    "                else:\n",
    "                    # Root node: SEM output + stochastic step\n",
    "                    val = sem_func([])\n",
    "                    if j not in categorical_idx:\n",
    "                        val += rng.normal(0, step)\n",
    "                    X_new[i, j] = val\n",
    "\n",
    "                updated.add(j)\n",
    "\n",
    "            # Propagate if any parent was updated\n",
    "            elif any(p in updated for p in parents):\n",
    "                parent_vals = X_new[i, parents]\n",
    "                val = sem_func(parent_vals)\n",
    "                if j not in categorical_idx:\n",
    "                    val += rng.normal(0, step/2)  # small noise even if parents updated\n",
    "                X_new[i, j] = val\n",
    "                updated.add(j)\n",
    "\n",
    "    return X_new\n"
   ],
   "id": "967d6cb957725c75"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# NSGA-III Counterfactual Generation\n",
    "\n",
    "import networkx as nx\n",
    "from pymoo.core.problem import Problem\n",
    "from pymoo.util.nds.non_dominated_sorting import NonDominatedSorting\n",
    "from typing import Callable, List, Dict\n",
    "\n",
    "# generates causally, valid, diverse, and efficient counterfactuals.\n",
    "\n",
    "# ----------------------------\n",
    "# CounterfactualGeneration Class\n",
    "# ----------------------------\n",
    "class CounterfactualGeneration(Problem):\n",
    "    def __init__(self, model, original_data, desired_class, actionable, immutable, sem_functions, causal_graph, target_column, stochastic=True):\n",
    "        self.model = model\n",
    "        self.original_data = original_data.reset_index(drop=True)\n",
    "        self.desired_class = desired_class\n",
    "        self.actionable = actionable\n",
    "        self.immutable = immutable\n",
    "        self.sem_functions = sem_functions\n",
    "        self.graph = causal_graph\n",
    "        self.target_column = target_column\n",
    "        self.stochastic = stochastic\n",
    "\n",
    "\n",
    "        self.features = list(original_data.columns)\n",
    "        if target_column in self.actionable:\n",
    "            self.actionable.remove(target_column)\n",
    "        if target_column in self.immutable:\n",
    "            self.immutable.remove(target_column)\n",
    "\n",
    "        self.feature_dim = len(self.features)\n",
    "        self.mutable_indices = [self.features.index(f) for f in actionable if f in self.features]\n",
    "\n",
    "\n",
    "        # bounds for all features\n",
    "        original_values = original_data.drop(columns=[target_column]).iloc[0].values\n",
    "        epsilon = 0.5\n",
    "        self.min_bounds = original_values - epsilon\n",
    "        self.max_bounds = original_values + epsilon\n",
    "\n",
    "        super().__init__(\n",
    "            n_var=len(original_data.columns) - 1,  # total features excluding target\n",
    "            n_obj=5,\n",
    "            n_constr = 2,  # validity, proximity constraint\n",
    "            xl=self.min_bounds,\n",
    "            xu=self.max_bounds\n",
    "                )\n",
    "\n",
    "\n",
    "    # Integrated _evaluate with constraint\n",
    "    # The constraint ensures the predicted probability for the desired class is >= 0.5\n",
    "    def _evaluate(self, X, out, *args, **kwargs):\n",
    "        x_orig = self.original_data.iloc[0].copy()\n",
    "        objective_matrix = []\n",
    "        chain_lengths = []\n",
    "        epsilon = 1e-8\n",
    "        pred_probs = []\n",
    "\n",
    "        # constraint for proximity\n",
    "        max_proximity = 3.0\n",
    "\n",
    "\n",
    "        for i, cf_vec in enumerate(X):\n",
    "            x_cf = x_orig.copy()\n",
    "            for j, feat_idx in enumerate(self.mutable_indices):\n",
    "                x_cf[self.features[feat_idx]] = cf_vec[j]\n",
    "\n",
    "            x_input = x_cf.drop(self.target_column).values.reshape(1, -1)\n",
    "            pred = self.model.predict(x_input)[0]\n",
    "            p1 = self.model.predict_proba(x_input)[0, self.desired_class]\n",
    "            pred_probs.append(p1)\n",
    "\n",
    "            # Validity\n",
    "            validity = 0 if pred == self.desired_class else 1\n",
    "\n",
    "            # Proximity\n",
    "            prox = np.linalg.norm(x_cf.drop(self.target_column).values - x_orig.drop(self.target_column).values)\n",
    "\n",
    "\n",
    "            # Effort\n",
    "            r_chain = self.build_recourse_chain(x_orig.drop(self.target_column).copy(),\n",
    "                                                x_cf.drop(self.target_column).copy())\n",
    "            effort = len(r_chain)\n",
    "            chain_lengths.append(effort)\n",
    "\n",
    "            # Plausibility\n",
    "            plausibility = 0.0\n",
    "            x_cf_raw = x_cf.copy()\n",
    "            for node in nx.topological_sort(self.graph):\n",
    "                if node in self.immutable or node in self.actionable:\n",
    "                    continue\n",
    "                parents = list(self.graph.predecessors(node))\n",
    "                if all(p in x_cf_raw for p in parents):\n",
    "                    inputs = x_cf_raw[parents].values.reshape(1, -1)\n",
    "                    sem_func, _ = self.sem_functions[node]\n",
    "                    result = sem_func(inputs)\n",
    "                    if isinstance(result, tuple) and len(result) == 2:\n",
    "                        mu, sigma = result\n",
    "                    else:\n",
    "                        mu = result\n",
    "                        sigma = 0.5\n",
    "                    mu_scalar = float(mu.flatten()[0])\n",
    "                    sampled_val = np.random.normal(mu_scalar, sigma) if self.stochastic else mu_scalar\n",
    "                    x_cf[node] = sampled_val\n",
    "                    nll = 0.5 * np.log(2 * np.pi * sigma ** 2) + ((sampled_val - mu_scalar) ** 2)/(2 * sigma ** 2)\n",
    "                    plausibility += float(nll.item())\n",
    "\n",
    "            # Recourse efficiency\n",
    "            downstream_features = set()\n",
    "            for node in self.actionable:\n",
    "                downstream_features.update(nx.descendants(self.graph, node))\n",
    "\n",
    "            features_without_target = list(x_orig.drop(self.target_column).index)\n",
    "            impact_features = list(downstream_features.intersection(features_without_target))\n",
    "\n",
    "            x_orig_model_inputs = x_orig.drop(self.target_column).values\n",
    "            x_cf_model_inputs = x_cf.drop(self.target_column).values\n",
    "\n",
    "            impact_indices = [features_without_target.index(f) for f in impact_features]\n",
    "            causal_impact = np.sum(np.abs(x_cf_model_inputs[impact_indices] - x_orig_model_inputs[impact_indices]))\n",
    "\n",
    "            # recourse_efficiency = effort / (causal_impact + epsilon)\n",
    "            recourse_efficiency = (causal_impact + epsilon) / effort\n",
    "\n",
    "\n",
    "            objective_matrix.append([\n",
    "                float(validity),\n",
    "                float(prox),\n",
    "                float(effort),\n",
    "                float(plausibility),\n",
    "                float(recourse_efficiency)\n",
    "                ])\n",
    "\n",
    "        out[\"F\"] = np.array(objective_matrix)\n",
    "        self._last_population = X\n",
    "        self._last_objectives = np.array(objective_matrix)\n",
    "        self._last_chains = chain_lengths\n",
    "\n",
    "        # Constraint:\n",
    "        # validity - desired class probability >= threshold\n",
    "        threshold = 0.5\n",
    "        pred_probs = np.array(pred_probs)\n",
    "        validity_constraint = np.maximum(0.0, threshold - pred_probs).reshape(-1, 1)\n",
    "        proximity_vals = np.array([obj[1] for obj in objective_matrix])\n",
    "        proximity_constraint = np.maximum(0.0, proximity_vals - max_proximity).reshape(-1, 1)\n",
    "\n",
    "        out[\"G\"] = np.hstack([validity_constraint, proximity_constraint])\n",
    "\n",
    "\n",
    "    # validates candidates by ensuring full causal consistency\n",
    "    def _apply_sem(self, x: pd.Series) -> pd.Series:\n",
    "        # Propagate changes forward in the DAG; ensures that non-actionable, non-immutable features are updated based on their causal parents\n",
    "        sorted_nodes = list(nx.topological_sort(self.graph))\n",
    "\n",
    "        for node in sorted_nodes:\n",
    "            if node in self.immutable or node in self.actionable:\n",
    "                continue\n",
    "\n",
    "            parents = list(self.graph.predecessors(node))\n",
    "            if all(p in x for p in parents):\n",
    "                inputs = x[parents].values.reshape(1, -1)\n",
    "                sem_func, _ = self.sem_functions[node]\n",
    "                result = sem_func(inputs)\n",
    "\n",
    "                if isinstance(result, tuple) and len(result) == 2:\n",
    "                    mu, sigma = result\n",
    "                else:\n",
    "                    mu = result\n",
    "                    d_sigma = 0.5\n",
    "                    sigma = d_sigma\n",
    "                x[node] = np.random.normal(mu[0], sigma) if self.stochastic else mu[0]\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "    # sequential chain using topological order\n",
    "    def build_recourse_chain(self, x_orig: pd.Series, x_cf: pd.Series):\n",
    "\n",
    "        changes = []\n",
    "        x_current = x_orig.copy()\n",
    "\n",
    "        for node in nx.topological_sort(self.graph):\n",
    "            if node in self.immutable:\n",
    "                continue\n",
    "\n",
    "            original_value = x_orig[node]\n",
    "            new_value = x_cf[node]\n",
    "\n",
    "            if not np.isclose(original_value, new_value):\n",
    "                changes.append({\n",
    "                    \"features\": node,\n",
    "                    \"original_value\": original_value,\n",
    "                    \"new_value\" : new_value,\n",
    "                   \"type\": \"direct_change\" if node in self.actionable else \"causal_propagation\"\n",
    "                    })\n",
    "\n",
    "                # SEM application to propagate descendants\n",
    "                x_current[node] = new_value\n",
    "                x_current = self._apply_sem(x_current)\n",
    "\n",
    "        return changes\n",
    "\n",
    "\n",
    "    # Pareto filtering and chain output\n",
    "    # return counterfactuals and recourse chains for pareto-optimal solutions as a list[Dict]\n",
    "    def extract_pareto_c_chains(self):\n",
    "        \"\"\"\n",
    "        Returns a list of dicts for each Pareto-optimal counterfactual:\n",
    "            - 'cf': the full counterfactual instance (pd.Series)\n",
    "            - 'chain': the recourse chain as List[(feature, new_value)]\n",
    "            - 'objectives': list of objective values [validity, prox, sparse, effort, plausibility, diversity]\n",
    "        \"\"\"\n",
    "\n",
    "        nd_indices = NonDominatedSorting().do(self._last_objectives, only_non_dominated_front=True)\n",
    "        x_orig = self.original_data.iloc[0].drop(self.target_column).copy()\n",
    "\n",
    "        pareto_c_chains = []\n",
    "        for i in nd_indices:\n",
    "            cf_values = self._last_population[i]\n",
    "            objective = self._last_objectives[i]\n",
    "\n",
    "            # reconstruct the full counterfactual\n",
    "            x_cf = x_orig.copy()\n",
    "            for j, feat_idx in enumerate(self.mutable_indices):\n",
    "                x_cf[self.features[feat_idx]] = cf_values[j]\n",
    "            x_cf = self._apply_sem(x_cf)\n",
    "\n",
    "            recourse_chain = self.build_recourse_chain(x_orig.copy(), x_cf.copy())\n",
    "            pareto_c_chains.append({\n",
    "                \"counterfactual\": x_cf,\n",
    "                \"recourse_chain\": recourse_chain,\n",
    "                \"objectives\": objective,\n",
    "                })\n",
    "\n",
    "        return pareto_c_chains\n"
   ],
   "id": "7dcbfdb0b5997b90"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from pymoo.algorithms.moo.nsga3 import NSGA3\n",
    "from pymoo.util.ref_dirs import get_reference_directions\n",
    "from pymoo.termination import get_termination\n",
    "from pymoo.core.callback import Callback\n",
    "from pymoo.optimize import minimize\n",
    "from pymoo.operators.crossover.sbx import SBX\n",
    "from pymoo.core.termination import Termination\n",
    "\n",
    "# ----------------------------\n",
    "# Logging callback\n",
    "# ----------------------------\n",
    "# tracks and visualize the search for optimal counterfactuals\n",
    "class LogCallback(Callback):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.data[\"gen\"] = []\n",
    "        self.data[\"n_evals\"] = []\n",
    "        self.data[\"opt\"] = []\n",
    "\n",
    "    def notify(self, algorithm):\n",
    "        self.data[\"gen\"].append(algorithm.n_gen)\n",
    "        self.data[\"n_evals\"].append(algorithm.evaluator.n_eval)\n",
    "        self.data[\"opt\"].append(algorithm.opt.get(\"F\"))\n",
    "\n",
    "# ----------------------------\n",
    "# Safe log odds\n",
    "# ----------------------------\n",
    "# transforms model outputs into a space where optimization is more effective or interpretable\n",
    "def safe_log_odds(prob):\n",
    "    eps = 1e-9\n",
    "    prob = np.clip(prob, eps, 1 - eps)\n",
    "    return np.log(prob / (1 - prob))\n",
    "\n",
    "# ----------------------------\n",
    "# Early Termination\n",
    "# ----------------------------\n",
    "class CounterfactualTermination(Termination):\n",
    "    def __init__(self, N=5, validity_index=0):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        N : int\n",
    "            Number of valid counterfactuals to stop after.\n",
    "        validity_index : int\n",
    "            Column index in F where validity is stored (default=0).\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.N = N\n",
    "        self.validity_index = validity_index\n",
    "\n",
    "    def _update(self, algorithm):\n",
    "        # Access the objective matrix\n",
    "        F = algorithm.opt.get(\"F\")\n",
    "\n",
    "        # Valid CFs = rows where validity == 0\n",
    "        valid_count = np.sum(F[:, self.validity_index] == 0)\n",
    "\n",
    "        # Stop when enough valid CFs are found\n",
    "        return valid_count >= self.N\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Population init near boundary\n",
    "# ----------------------------\n",
    "def init_population_near_boundary(model, X_pool, original_instance, actionable_features, n_pop=100,\n",
    "                           eps=1, epsilon=0.05, noise_scale=0.01, rng=None):\n",
    "    \"\"\"\n",
    "    Initialize a population for counterfactual search that is both:\n",
    "      1. Close to the original instance (low proximity)\n",
    "      2. Near the decision boundary (predicted probability ≈ 0.5)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : scikit-learn model\n",
    "        Trained model with predict_proba.\n",
    "    X_pool : pd.DataFrame\n",
    "        Full feature set (to select boundary candidates).\n",
    "    original_instance : pd.Series\n",
    "        Original instance for proximity reference.\n",
    "    actionable_features : list\n",
    "        Names of actionable features.\n",
    "    n_pop : int\n",
    "        Population size.\n",
    "    eps : float\n",
    "        Probability window around 0.5 for boundary selection.\n",
    "    epsilon : float\n",
    "        Max deviation from original instance for proximity.\n",
    "    noise_scale : float\n",
    "        Gaussian noise scale for actionable feature perturbation.\n",
    "    rng : int or np.random.Generator\n",
    "        Random seed or generator.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    X_init : np.ndarray\n",
    "        Initialized population of shape (n_pop, n_features).\n",
    "    \"\"\"\n",
    "    if rng is None:\n",
    "        rng = np.random.default_rng()\n",
    "    elif isinstance(rng, int):\n",
    "        rng = np.random.default_rng(rng)\n",
    "\n",
    "    # Step 1: Select boundary-near candidates\n",
    "    X_full = X_pool.copy().values\n",
    "    p1 = model.predict_proba(X_full)[:, 1]\n",
    "    mask = (p1 >= 0.5 - eps) & (p1 <= 0.5 + eps)\n",
    "    candidates = X_full[mask] if np.any(mask) else X_full\n",
    "    n_candidates = len(candidates)\n",
    "\n",
    "    # Step 2: Initialize population\n",
    "    X_init = np.zeros((n_pop, X_full.shape[1]))\n",
    "    X_init = X_init.astype(np.float32)\n",
    "    orig_values = original_instance.values\n",
    "\n",
    "    for i in range(n_pop):\n",
    "        # Pick a random boundary candidate\n",
    "        idx = rng.integers(0, n_candidates)\n",
    "        x = candidates[idx].copy()\n",
    "\n",
    "        # Perturb actionable features:\n",
    "        for f in actionable_features:\n",
    "            col_idx = X_pool.columns.get_loc(f)\n",
    "            # Stay close to the original (low proximity) + small Gaussian noise\n",
    "            x[col_idx] = orig_values[col_idx] + rng.uniform(-epsilon, epsilon) \\\n",
    "                         + rng.normal(0, noise_scale)\n",
    "\n",
    "        X_init[i] = x\n",
    "\n",
    "    return X_init\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Counterfactual generation\n",
    "# ----------------------------\n",
    "def generate_counterfactual(input_instance, n_generations=100, n_partitions=5, seed=42, N= 4, multiple_instance_mode = False):\n",
    "    model = input_instance.model\n",
    "    x_input = input_instance.original_data.drop(input_instance.target_column, axis=1).values.reshape(1, -1)\n",
    "\n",
    "    # --- Initial probability & log-odds ---\n",
    "    if hasattr(model, \"predict_proba\"):\n",
    "        proba = model.predict_proba(x_input)[0]\n",
    "        desired_prob = proba[input_instance.desired_class]\n",
    "    elif hasattr(model, \"decision_function\"):\n",
    "        score = model.decision_function(x_input)\n",
    "        if score.ndim > 1:\n",
    "            score = score[:, input_instance.desired_class]\n",
    "        desired_prob = 1 / (1 + np.exp(-score))\n",
    "    else:\n",
    "        raise ValueError(\"Model does not support probability or decision_function output.\")\n",
    "\n",
    "    print(f\"Initial probability for desired class {input_instance.desired_class}: {desired_prob:.4f}\")\n",
    "    log_odds = safe_log_odds(desired_prob)\n",
    "    print(f\"Log-odds for desired class {input_instance.desired_class}: {log_odds:.4f}\")\n",
    "\n",
    "    if abs(log_odds) > 0.2:\n",
    "        print(\"⚠ Far from decision boundary — increasing search budget.\")\n",
    "        n_generations = int(n_generations * 1.5)\n",
    "\n",
    "    # NSGA-III setup\n",
    "    n_obj = input_instance.n_obj\n",
    "    ref_dirs = get_reference_directions(\"das-dennis\", n_dim=n_obj, n_partitions=n_partitions)\n",
    "    pop_size = len(ref_dirs)\n",
    "    print(f\"[INFO] Using {pop_size} reference directions for {n_obj} objectives (n_partitions={n_partitions})\")\n",
    "\n",
    "    # Initial population\n",
    "    x_orig = input_instance.original_data.drop(columns=[input_instance.target_column]).iloc[0].values\n",
    "    rng = np.random.default_rng(seed)\n",
    "    initial_population = np.tile(x_orig, (pop_size, 1)).astype(float)\n",
    "\n",
    "    for idx in input_instance.mutable_indices:\n",
    "        initial_population[:, idx] += rng.normal(0, 0.01, size=pop_size)\n",
    "\n",
    "    actionable_idx = input_instance.mutable_indices\n",
    "\n",
    "    # NSGA-III algorithm\n",
    "    algorithm = NSGA3(\n",
    "        ref_dirs=ref_dirs, pop_size=pop_size, sampling=initial_population,\n",
    "        crossover=SBX(prob=0.9, eta=15),\n",
    "        mutation=lambda problem, X, **kwargs: batch_mutate_diverse(\n",
    "            X,\n",
    "            actionable_idx=actionable_idx,\n",
    "            sem_functions=sem_functions,\n",
    "            step=0.02,\n",
    "            rng=np.random.default_rng(seed),\n",
    "            mutation_rate=0.5,\n",
    "            categorical_idx={0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23}\n",
    "        ),\n",
    "        eliminate_duplicates=True\n",
    "    )\n",
    "\n",
    "    termination = CounterfactualTermination(N=N, validity_index=0)\n",
    "    callback = LogCallback()\n",
    "\n",
    "    result = minimize(\n",
    "        problem=input_instance,\n",
    "        algorithm=algorithm,\n",
    "        termination=termination,\n",
    "        seed=seed,\n",
    "        callback=callback,\n",
    "        save_history=False,\n",
    "        verbose=False\n",
    "    )\n",
    "    # ----------------------------\n",
    "    # Post-hoc filtering\n",
    "    # ----------------------------\n",
    "     # --- Extract valid counterfactuals ---\n",
    "    cf_results = input_instance.extract_pareto_c_chains()\n",
    "\n",
    "    if len(cf_results) == 0:\n",
    "        print(\"⚠ No valid counterfactuals found.\")\n",
    "        return None, None\n",
    "\n",
    "    # Use dict keys instead of attributes\n",
    "    pop = np.array([cf['counterfactual'] for cf in cf_results])\n",
    "    objs = np.array([cf['objectives'] for cf in cf_results])\n",
    "\n",
    "    # Keep only valid counterfactuals (validity = 0 at index 0)\n",
    "    valid_mask = objs[:, 0] == 0\n",
    "    pop = pop[valid_mask]\n",
    "    objs = objs[valid_mask]\n",
    "\n",
    "    if len(pop) == 0:\n",
    "        print(\"⚠ No valid counterfactuals after filtering.\")\n",
    "        return None, None\n",
    "\n",
    "    # Apply N-constraint and multiple-instance mode\n",
    "    cap = int(N * 2) if multiple_instance_mode else N\n",
    "    if len(pop) > cap:\n",
    "        # Sort by proximity (second objective)\n",
    "        sorted_idx = np.argsort(objs[:, 1])\n",
    "        pop = pop[sorted_idx][:cap]\n",
    "        objs = objs[sorted_idx][:cap]\n",
    "\n",
    "    print(f\"Returning {len(pop)} valid counterfactual(s) from extract_pareto_c_chains.\")\n",
    "    return pop, objs\n",
    "\n"
   ],
   "id": "f21bc2e811ec8be7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# prep the data for counterfactual generation\n",
    "x_test['default payment next month'] = y_test.values\n",
    "print(len(x_test))\n",
    "\n",
    "# x_test.head(5)\n",
    "\n",
    "# single instance\n",
    "cf_data_instance = x_test[x_test['default payment next month'] == 0].iloc[[4]].copy()\n",
    "#multiple instance\n",
    "cf_data_instances = x_test[x_test['default payment next month'] == 0].iloc[:200].copy()\n"
   ],
   "id": "64989169a5e6536b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Counterfactual Generation\n",
    "# warning occurs because the prediction is made without the column name, while it was trained with the column name\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"X does not have valid feature names\")\n",
    "\n",
    "input_instance = CounterfactualGeneration(model = rf_model,\n",
    "                                          original_data= cf_data_instance,\n",
    "                                          desired_class = 1,\n",
    "                                          actionable = actionable_f,\n",
    "                                          immutable = immutable_f,\n",
    "                                          sem_functions = sem_functions,\n",
    "                                          causal_graph = DAG_graph,\n",
    "                                          target_column = \"default payment next month\",\n",
    "                                          stochastic = True )\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "cf_result = generate_counterfactual(input_instance)\n",
    "end_time = time.time()\n",
    "print(f\"Time taken to generate counterfactuals: {end_time - start_time}\")\n",
    "\n",
    "cf_results = input_instance.extract_pareto_c_chains()"
   ],
   "id": "7829587cc2643463"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "for idx, cf_entry in enumerate(cf_results, start=1):\n",
    "    print(f\"=== Counterfactual {idx} ===\")\n",
    "\n",
    "    # Print counterfactual values\n",
    "    cf_vector = cf_entry.get(\"counterfactual\", None)\n",
    "    if cf_vector is not None:\n",
    "        print(\"Counterfactual vector:\")\n",
    "        print(cf_vector.to_string(float_format=lambda x: f\"{x:.6f}\"))\n",
    "    else:\n",
    "        print(\"No counterfactual data.\")\n",
    "\n",
    "    print(\"\\nRecourse Chain:\")\n",
    "    recourse_chain = cf_entry.get(\"recourse_chain\", None)\n",
    "    if not recourse_chain:\n",
    "        print(\"  None\")\n",
    "    else:\n",
    "        for step in recourse_chain:\n",
    "            feature = step.get('features')\n",
    "            old_val = step.get('original_value')\n",
    "            new_val = step.get('new_value')\n",
    "            change_type = step.get('type')\n",
    "            print(f\"  - {feature}: {old_val:.6f} -> {new_val:.6f} ({change_type})\")\n",
    "\n",
    "    print(\"\\nObjectives:\", cf_entry.get(\"objectives\", None))\n",
    "    print()"
   ],
   "id": "dd8724d03ecb161"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# best counterfactual with validity as zero (successful counterfactuals)\n",
    "\n",
    "def is_pareto_efficient(costs):\n",
    "    \"\"\"\n",
    "    Find the Pareto-efficient points.\n",
    "    Returns a boolean array indicating whether each point is Pareto efficient.\n",
    "    \"\"\"\n",
    "    is_efficient = np.ones(costs.shape[0], dtype=bool)\n",
    "    for i, c in enumerate(costs):\n",
    "        if is_efficient[i]:\n",
    "            is_efficient[is_efficient] = (\n",
    "                np.any(costs[is_efficient] < c, axis=1) |\n",
    "                np.all(costs[is_efficient] == c, axis=1)\n",
    "            )\n",
    "            is_efficient[i] = True\n",
    "    return is_efficient\n",
    "\n",
    "# Extract all objective vectors from cf_chains\n",
    "objectives_array = np.array([cf['objectives'] for cf in cf_results])\n",
    "\n",
    "# Find Pareto-efficient CFs\n",
    "pareto_mask = is_pareto_efficient(objectives_array)\n",
    "\n",
    "# Filter further for validity == 1\n",
    "# Assuming 'validity' is the first objective in the list\n",
    "validity_mask = objectives_array[:, 0] == 0\n",
    "\n",
    "# Combined filter\n",
    "final_mask = pareto_mask & validity_mask\n",
    "\n",
    "print(f\"\\nPrinting only Pareto-efficient counterfactuals with validity = 0 ({final_mask.sum()} found):\\n\")\n",
    "\n",
    "for idx, (cf_entry, is_best) in enumerate(zip(cf_results, final_mask), start=1):\n",
    "    if not is_best:\n",
    "        continue\n",
    "\n",
    "    print(f\"=== Counterfactual {idx} ===\")\n",
    "\n",
    "    # Counterfactual vector\n",
    "    cf_vector = cf_entry.get(\"counterfactual\", None)\n",
    "    if cf_vector is not None:\n",
    "        print(\"Counterfactual vector:\")\n",
    "        if hasattr(cf_vector, \"to_string\"):\n",
    "            print(cf_vector.to_string(float_format=lambda x: f\"{x:.6f}\"))\n",
    "        else:\n",
    "            print(cf_vector)\n",
    "\n",
    "    print(\"\\nRecourse Chain:\")\n",
    "    recourse_chain = cf_entry.get(\"recourse_chain\", None)\n",
    "    if not recourse_chain:\n",
    "        print(\"  None\")\n",
    "    else:\n",
    "        for step in recourse_chain:\n",
    "            if isinstance(step, dict):\n",
    "                feature = step.get('features')\n",
    "                old_val = step.get('original_value')\n",
    "                new_val = step.get('new_value')\n",
    "                change_type = step.get('type', '')\n",
    "                print(f\"  - {feature}: {old_val:.6f} -> {new_val:.6f} {change_type}\")\n",
    "            else:\n",
    "                print(f\"  Unexpected step format: {step}\")\n",
    "\n",
    "    print(\"\\nObjectives:\", cf_entry.get(\"objectives\", None))\n",
    "    print(\"-\" * 50)\n"
   ],
   "id": "5dd415885e17d573"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Decode and inverse-transform generated counterfactuals\n",
    "decoded_cf_list = []\n",
    "\n",
    "target_column = \"risk\"\n",
    "desired_class = 1\n",
    "\n",
    "for cf_entry, is_best in zip(cf_results, final_mask):\n",
    "    if not is_best:\n",
    "        continue\n",
    "\n",
    "    cf_vector = cf_entry.get(\"counterfactual\", None)\n",
    "    if cf_vector is None:\n",
    "        continue\n",
    "\n",
    "    data_columns = dc_data.columns\n",
    "\n",
    "    # Convert to array\n",
    "    cf_data_array = np.array(cf_vector).reshape(1, -1)\n",
    "\n",
    "    # Inverse transform to original scale\n",
    "    cf_data_unscaled = scaler.inverse_transform(cf_data_array)\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    counterfactual_df = pd.DataFrame(cf_data_unscaled, columns=data_columns)\n",
    "\n",
    "    decoded_cf_list.append(counterfactual_df)\n",
    "\n",
    "# Combine all into one DataFrame\n",
    "decoded_cf_df = pd.concat(decoded_cf_list, ignore_index=True)"
   ],
   "id": "7597755c4922df"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "decoded_cf_df",
   "id": "47f59982b2468a00"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# decode the original instance\n",
    "cf_data_instance = x_test[x_test['default payment next month'] == 0].iloc[[4]].copy()\n",
    "original_data= cf_data_instance\n",
    "\n",
    "# Drop the target column before inverse transform\n",
    "features_only = cf_data_instance.drop(columns=['default payment next month'])\n",
    "\n",
    "# Inverse transform\n",
    "original_data_unscaled = scaler.inverse_transform(features_only)\n",
    "\n",
    "# Convert back to DataFrame for readability\n",
    "original_decoded = pd.DataFrame(original_data_unscaled, columns=features_only.columns)"
   ],
   "id": "20ba865bbb7ee0ec"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "original_decoded\n",
   "id": "ec964135c58e30dc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# visible highlight the changes\n",
    "\n",
    "def highlight_changes(val):\n",
    "    if isinstance(val, str) and \"→\" in val:\n",
    "        return \"background-color: #d4edda; color: green; font-weight: bold;\"  # light green\n",
    "    return \"\"\n",
    "\n",
    "counterfactual_gen = decoded_cf_df\n",
    "# Repeat the original row 22 times to match CF rows\n",
    "original_repeated = pd.concat([original_decoded]*counterfactual_gen.shape[0], ignore_index=True)\n",
    "\n",
    "# Convert both DataFrames to string\n",
    "original_str = original_repeated.astype(str)\n",
    "cf_str = decoded_cf_df.astype(str)\n",
    "\n",
    "# Align columns just in case\n",
    "original_str, cf_str = original_str.align(cf_str, join='inner', axis=1)\n",
    "\n",
    "# Create comparison DataFrame to show old → new values where different\n",
    "comparison_df = original_str.copy()\n",
    "\n",
    "for col in comparison_df.columns:\n",
    "    diff_mask = original_str[col] != cf_str[col]\n",
    "    comparison_df.loc[diff_mask, col] = original_str.loc[diff_mask, col] + \" → \" + cf_str.loc[diff_mask, col]\n",
    "\n",
    "# Now apply highlight with Styler\n",
    "styled_df = comparison_df.style.applymap(highlight_changes)\n",
    "\n",
    "styled_df  # In Jupyter this will display the styled DataFrame\n"
   ],
   "id": "172c22039d0f169a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Objectives\n",
    "\n",
    "objectives_list = []\n",
    "\n",
    "for cf_entry, is_best in zip(cf_results, final_mask):\n",
    "    if not is_best:\n",
    "        continue\n",
    "\n",
    "    objectives = cf_entry.get(\"objectives\", None)\n",
    "    if objectives is None:\n",
    "        continue\n",
    "\n",
    "    obj_names = ['validity', 'proximity', 'effort', 'plausibility', 'recourse_efficiency']\n",
    "    obj_dict = {name: float(obj) for name, obj in zip(obj_names, objectives)}\n",
    "    objectives_list.append(obj_dict)\n",
    "\n",
    "# Convert list of dicts to DataFrame\n",
    "objectives_df = pd.DataFrame(objectives_list)\n",
    "objectives_df\n"
   ],
   "id": "915087b5ecb6490a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "550584fac7a4b94e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
